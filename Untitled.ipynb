{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdc80b5-a0dd-4355-9ebf-032b64cd6658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_refusal_edit(\n",
    "    prompts,\n",
    "    ablate=True,  # True = subtract vector, False = add vector\n",
    "    scale=1.0,\n",
    "    max_new_tokens=150,\n",
    "    tokenizer_kwargs={},\n",
    "    layer = \"layer_21\", # [layer_21]\n",
    "    step = 0\n",
    "):\n",
    "    hooks = []\n",
    "\n",
    "    def project_orthogonal(hidden, vec, scale):\n",
    "        # vec should be 1D, normalize explicitly\n",
    "        vec = vec / vec.norm()\n",
    "        coeff = (hidden * vec).sum(dim=-1, keepdim=True)\n",
    "        return hidden - scale * coeff * vec\n",
    "\n",
    "\n",
    "    def edit_fn(module, input, output, layer_key):\n",
    "        hidden = output\n",
    "\n",
    "        \n",
    "        # print(scale)\n",
    "        if layer_key in refusal_vectors:\n",
    "            vec = refusal_vectors[layer][step].to(hidden.device)   ## cahnged from layer_key\n",
    "            if ablate: \n",
    "                hidden = project_orthogonal(hidden, vec, scale)\n",
    "            else:\n",
    "                if layer_key != layer: #lol check this in paper\n",
    "                    return hidden\n",
    "                hidden = hidden + scale * vec\n",
    "        return hidden\n",
    "    \n",
    "    # Register hooks for each layer\n",
    "    for i, block in enumerate(model.model.layers):\n",
    "        key = f\"layer_{i}\"\n",
    "        if key in refusal_vectors:\n",
    "            h = block.register_forward_hook(partial(edit_fn, layer_key=key))\n",
    "            hooks.append(h)\n",
    "\n",
    "    try:\n",
    "        convs = [\n",
    "            tokenizer.apply_chat_template(\n",
    "                [{\"role\": \"user\", \"content\": p}],\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=False,\n",
    "                enable_thinking=False,\n",
    "                **tokenizer_kwargs,\n",
    "            )\n",
    "            for p in prompts\n",
    "        ]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            convs,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        )\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward once to get logits at the last *input* token for each example\n",
    "            input_outputs = model(**inputs)  # logits: [B, T, V]\n",
    "            logits = input_outputs.logits\n",
    "\n",
    "            # Find last non-pad input token index per example\n",
    "            # attention_mask: [B, T], sum-1 gives last index\n",
    "            last_indices = inputs[\"attention_mask\"].sum(dim=1) - 1  # [B]\n",
    "\n",
    "            # Gather logits at those indices => shape [B, V]\n",
    "            # Build indices for gather\n",
    "            bsz, _, vocab_size = logits.shape\n",
    "            gather_idx = last_indices.view(bsz, 1, 1).expand(bsz, 1, vocab_size)  # [B,1,V]\n",
    "            last_input_logits = torch.gather(logits, dim=1, index=gather_idx).squeeze(1)  # [B,V]\n",
    "\n",
    "            # Generate as usual (batched)\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                return_dict_in_generate=True,\n",
    "                output_logits=True,\n",
    "            )\n",
    "\n",
    "        # Attach the batched last-input logits\n",
    "        output.last_input_logits = last_input_logits  # [batch_size, vocab_size]\n",
    "        return output\n",
    "        \n",
    "    finally:\n",
    "        # Ensure cleanup even if generation fails\n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "\n",
    "    # Decode\n",
    "    return output_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
