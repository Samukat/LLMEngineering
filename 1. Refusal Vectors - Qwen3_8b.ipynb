{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b202a973-a229-41d8-90f5-d4ee02fc0c99",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbf57f23-3394-49ff-81e1-bcec7d4c53d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "from transformers import GenerationConfig\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import accelerate\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32f751-0f36-4e09-a40d-d6a9e82ca46e",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cd1cd3c-88f8-4137-9efa-42842af18353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63d6bd182fb4b1ab40213fa67657100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda:1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen3-8B\", \n",
    "    trust_remote_code=True,\n",
    "    # device_map=\"auto\",   # Hugging Face accelerates across multiple GPUs\n",
    "    dtype=\"auto\"   # or torch.float16 / bfloat16\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b73dae40-af7e-4550-b73d-a7e9debeee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data():\n",
    "    harm_ds = load_dataset(\"walledai/AdvBench\")\n",
    "    alpaca_dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "    malicious_ds = load_dataset(\"walledai/MaliciousInstruct\")\n",
    "\n",
    "    harm = harm_ds[\"train\"].select_columns([\"prompt\"]).select(range(150))\n",
    "    clean = alpaca_dataset[\"train\"].select_columns([\"instruction\", \"input\"]).select(range(200))\n",
    "    malicious = malicious_ds[\"train\"].select_columns([\"prompt\"]).select(range(50))\n",
    "\n",
    "    harm_ls = [p['prompt'] for p in harm]\n",
    "    clean_ls = [f\"{p['instruction']} {p['input']}\".strip() for p in clean]\n",
    "    malicious_ls = [p['prompt'] for p in malicious]\n",
    "    \n",
    "    return (harm_ls + malicious_ls, clean_ls)\n",
    "\n",
    "harm_test, clean_test = load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc28612b-92e8-40cf-b90a-ee8425cfa5c2",
   "metadata": {},
   "source": [
    "## Functions to get activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3be5aec-a37d-408b-b986-c45dc5334032",
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in model.modules():\n",
    "    module._forward_hooks.clear()\n",
    "    module._forward_pre_hooks.clear()\n",
    "    module._backward_hooks.clear()\n",
    "\n",
    "def get_full_block_activations(insts, tokenizer_kwargs={}, gen_kwargs={\"max_new_tokens\":4}):\n",
    "    temp_hidden_state_dict = defaultdict(lambda: defaultdict(list))\n",
    "    step_counter = [-1]  # list so it’s mutable and shared in closures\n",
    "\n",
    "    def make_hook_fn(temp_hidden_state_dict, step_counter, trailing_tokens=3):\n",
    "        def hook_fn(module, input, output, key):\n",
    "            activations = output[0].detach().cpu()  # (batch, seq_len, hidden_dim)\n",
    "            seq_len = activations.shape[0]\n",
    "    \n",
    "            if seq_len > 1:\n",
    "                n_keep = min(trailing_tokens, seq_len)\n",
    "                for i in range(n_keep):\n",
    "                    step_label = 0 if i == 0 else -i\n",
    "                    temp_hidden_state_dict[key][step_label].append(activations[-(i+1), :])\n",
    "            else:\n",
    "                # Generation: only 1 token\n",
    "                temp_hidden_state_dict[key][step_counter[0]].append(activations[0])\n",
    "        return hook_fn\n",
    "\n",
    "\n",
    "    orig_forward = model.forward\n",
    "    def forward_with_step_tracking(*args, **kwargs):\n",
    "        step_counter[0] += 1\n",
    "        return orig_forward(*args, **kwargs)\n",
    "\n",
    "    model.forward = forward_with_step_tracking\n",
    "\n",
    "    # Register hooks\n",
    "    hook_dict = {}\n",
    "    for i, block in enumerate(model.model.layers):\n",
    "        hook_dict[f\"layer_{i}\"] = block.register_forward_hook(\n",
    "            partial(make_hook_fn(temp_hidden_state_dict, step_counter, trailing_tokens=5), key=f\"layer_{i}\")\n",
    "        )\n",
    "\n",
    "    for inst in tqdm(insts):\n",
    "        step_counter[0] = -1  # reset per prompt\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": inst}],\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "            \n",
    "        )\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", **tokenizer_kwargs)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "    # Remove hooks & restore forward\n",
    "    for k in hook_dict:\n",
    "        hook_dict[k].remove()\n",
    "    model.forward = orig_forward\n",
    "\n",
    "    return temp_hidden_state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8a8d7a1-8374-43ea-a15d-e8ebde277051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:45<00:00,  4.39it/s]\n",
      "100%|██████████| 200/200 [00:46<00:00,  4.33it/s]\n"
     ]
    }
   ],
   "source": [
    "harmful_vectors = get_full_block_activations(harm_test)\n",
    "harmless_vectors = get_full_block_activations(clean_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6024cd6-e62d-4e97-8c81-b265786b6daf",
   "metadata": {},
   "source": [
    "## Subtracting the Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3266a1a1-3284-4368-b0ec-e7185aa01337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:01<00:00, 21.70it/s]\n"
     ]
    }
   ],
   "source": [
    "mean_harm = defaultdict(dict)\n",
    "mean_clean = defaultdict(dict)\n",
    "refusal_vectors = defaultdict(dict)\n",
    "\n",
    "for layer in tqdm(harmful_vectors.keys()):\n",
    "    for step in harmful_vectors[layer].keys():\n",
    "        # Stack once per step per layer\n",
    "        harm_stack = torch.stack(harmful_vectors[layer][step], dim=0)  # shape (N, ...)\n",
    "        clean_stack = torch.stack(harmless_vectors[layer][step], dim=0)\n",
    "\n",
    "        # Compute mean once\n",
    "        mean_harm_vec = harm_stack.mean(dim=0)\n",
    "        mean_clean_vec = clean_stack.mean(dim=0)\n",
    "\n",
    "        mean_harm[layer][step] = mean_harm_vec\n",
    "        mean_clean[layer][step] = mean_clean_vec\n",
    "        refusal_vectors[layer][step] = mean_harm_vec - mean_clean_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405b8083-5aca-4fce-87d0-a48277da4b42",
   "metadata": {},
   "source": [
    "## Saving The Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3c87c48-4ae0-4a5a-9788-e9f1b90a909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_candiate_vectors(filename, temp_hidden_state_dict):\n",
    "    # Convert nested defaultdicts to normal dicts recursively\n",
    "    def convert(d):\n",
    "        if isinstance(d, defaultdict):\n",
    "            d = {k: convert(v) for k, v in d.items()}\n",
    "        elif isinstance(d, list):\n",
    "            # List of tensors: convert tensors to CPU if needed\n",
    "            d = [t.cpu() if isinstance(t, torch.Tensor) else t for t in d]\n",
    "        return d\n",
    "    \n",
    "    normal_dict = convert(temp_hidden_state_dict)\n",
    "    torch.save(normal_dict, filename)\n",
    "\n",
    "save_candiate_vectors(\"refusal_vectors.pt\", refusal_vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
